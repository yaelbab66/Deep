{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ZjbOhbyspOBb"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yaelbab66/Deep/blob/main/Copy_of_Assignment02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 2\n",
        "#  Transformers for Vision"
      ],
      "metadata": {
        "id": "Adc2AAUqx3kV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Student 1 ID and name,**\n",
        "\n",
        "**Student 2 ID and name**"
      ],
      "metadata": {
        "id": "ioc2Z913h0tj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objective\n"
      ],
      "metadata": {
        "id": "stG8tsD_x62N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment, the goal is to implement a Transformer architecture for image classification OxfordIIITPet dataset."
      ],
      "metadata": {
        "id": "_xsooI4-chot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instructions\n",
        "\n"
      ],
      "metadata": {
        "id": "OQ19KxOfx8rN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The assignment includes two parts, the first is filling in the missing code in the provided code cells. The second part deals with testing and comparing different implementations.\n",
        "\n",
        "### Submission Guidelines:\n",
        "*   Assignments are done in pairs, include both ids in the filename when submitting (e.g. *HW02_123456789_123456789.ipynb*).\n",
        "*   Submit a Jupyter notebook containing your code modifications, comments, and analysis.\n",
        "*   Include visualizations, graphs, or plots to support your analysis where needed.\n",
        "*   Provide a conclusion summarizing your findings, challenges faced, and potential future improvements.\n",
        "\n",
        "\n",
        "### Important Notes:\n",
        "\n",
        "*  Ensure clarity in code comments and explanations for better understanding.\n",
        "*  Experiment, analyze, and document your observations throughout the assignment.\n",
        "*  Feel free to train on Colab GPU (see example in practice 4 notebook).\n",
        "*  If answering open ended questions in Markdown is difficult, you can attatch a doc/pdf file to your submittion which holds any/all explanations. Just make sure it is aligned with the code somehow.\n",
        "*  Feel free to seek clarification on any aspect of the assignment via forum or email."
      ],
      "metadata": {
        "id": "1CQWFjFbm6Sj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformers for Image Classification"
      ],
      "metadata": {
        "id": "OksOf2patKhC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformers have been originally proposed to process sets since it is a permutation-equivariant architecture, i.e., producing the same output permuted if the input is permuted. To apply Transformers to sequences, we have simply added a positional encoding to the input feature vectors, and the model learned by itself what to do with it. So, why not do the same thing on images? This is exactly what [Alexey Dosovitskiy et al.](https://openreview.net/pdf?id=YicbFdNTTy) proposed in their paper “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale”. Specifically, the Vision Transformer is a model for image classification that views images as sequences of smaller patches. As a preprocessing step, we split an image of, for example 48x48 pixels into 9 16x16 patches. Each of those patches is considered to be a “word”/“token” and projected to a feature space. With adding positional encodings and a token for classification on top, we can apply a Transformer as usual to this sequence and start training it for our task. A nice GIF visualization of the architecture is shown below."
      ],
      "metadata": {
        "id": "WNR0BNBTtO8M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center width=\"100%\"><img src=\"https://github.com/lucidrains/vit-pytorch/blob/main/images/vit.gif?raw=true\" width=\"800px\"></center>"
      ],
      "metadata": {
        "id": "JC9P37gZvL3H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports & Device setup"
      ],
      "metadata": {
        "id": "ZjbOhbyspOBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# arrange any/all imports here\n",
        "\n",
        "#for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "## Torchvision\n",
        "import torchvision\n",
        "from torchvision.datasets import OxfordIIITPet\n",
        "from torchvision import transforms\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "DATASET_PATH = \"data\""
      ],
      "metadata": {
        "id": "UmywJV9WpSoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 1: The Data\n",
        "\n"
      ],
      "metadata": {
        "id": "5INLj_V4qbft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we will load, explore and prepare the dataset for training.\n",
        "\n",
        "You are given a dataset of colored images, **in different sizes**, representing cats and dogs, which are classified into 37 different breeds (of both cats and dogs). The dataset is called *OxfordIIITPet* you can read more about it [here](https://www.robots.ox.ac.uk/~vgg/data/pets/). The dataset will be downloaded to local enviroment using the `tourchvision` library and split into the train-eval-test sets. Your tasks in this section are:\n",
        "\n",
        "> 1. Describe how you would preprocess the data for a vision transformer explain your choice of transofrmation. Implement the preprocessing of your choice in the `train_transform` and `test_transform` code section below. Are they identical? why or why not.\n",
        "\n",
        "> 2. Create a data loader for the trian, eval, and test sets. Explain your choice of `batch_size`."
      ],
      "metadata": {
        "id": "3gj6spKOJdiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations for the images\n",
        "# First we need to resize all images\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize to ViT input size\n",
        "    transforms.RandomHorizontalFlip(p=0.5),  # Randomly flip images for augmentation\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Random color jitter\n",
        "    transforms.ToTensor(),  # Convert to tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "                                      #TODO\n",
        "                                      ])"
      ],
      "metadata": {
        "id": "q98sbnBn4BMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the train-eval dataset\n",
        "dataset = OxfordIIITPet(root=DATASET_PATH, split=\"trainval\", transform=train_transform, target_types=\"category\", download=True)\n",
        "# Load the test set\n",
        "test_set = OxfordIIITPet(root=DATASET_PATH, split=\"test\", transform=test_transform, target_types=\"category\", download=True)\n",
        "\n",
        "# Split into training and validation randomly\n",
        "train_size = int(0.9 * len(dataset))  # 90% for training\n",
        "val_size = len(dataset) - train_size  # 10% for validation\n",
        "train_set, val_set = random_split(dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "# Update validation set with the test transform\n",
        "val_set.dataset.transform = test_transform"
      ],
      "metadata": {
        "id": "LhT6mQQh4CHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = #TODO\n",
        "\n",
        "# Data loaders\n",
        "train_loader =\n",
        "val_loader =\n",
        "test_loader =\n",
        "\n",
        "print(f\"Number of training samples: {len(train_set)}\")\n",
        "print(f\"Number of validation samples: {len(val_set)}\")\n",
        "print(f\"Number of test samples: {len(test_set)}\")"
      ],
      "metadata": {
        "id": "kHvSWFBk4F3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print 4 images to see what they include"
      ],
      "metadata": {
        "id": "NwQ14Y-m4VEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize some examples\n",
        "NUM_IMAGES = 4\n",
        "example_images = torch.stack([val_set[idx][0] for idx in range(NUM_IMAGES)], dim=0)\n",
        "img_grid = torchvision.utils.make_grid(example_images, nrow=4, normalize=True, pad_value=0.9)\n",
        "img_grid = img_grid.permute(1, 2, 0)\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.title(\"Image examples of the OxfordIIITPet dataset\")\n",
        "plt.imshow(img_grid)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "1qBvRSlE4WsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2: Prepare Patches\n",
        "\n"
      ],
      "metadata": {
        "id": "Tg1mioQEsQEo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vision Transformers (ViTs) begin by splitting input images into smaller patches. This approach enables ViTs to process images as sequences of fixed-size patches rather than whole images. Just as words become tokens in natural language processing (NLP), each image patch becomes a token for processing.\n",
        "\n",
        "The image patching process involves two steps:\n",
        "\n",
        "- **Image Partitioning**: Divide the image into equal, non-overlapping patches (for example 8×8 pixels).\n",
        "- **Flattening Patches**: Convert each patch into a 1D vector to create individual tokens.\n",
        "\n",
        "The code sections below demonstrate how to create these patches from an input image and patch size. An image of size $N\\times N$ is split into $(N/M)^2$ patches of size $M\\times M$. These patches serve as the input \"words\" to the Transformer.\n",
        "\n",
        "Review the code carefully and experiment with the example sections below to understand how the patches are created. Then answer the following question:\n",
        "\n",
        "> What patch size would you choose for your pipeline? Your answer should consider the chosen image size. How do you think patch size (smaller or larger) affects the pipeline's performance?"
      ],
      "metadata": {
        "id": "uGS9YrccpIvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def img_to_patch(x, patch_size, flatten_channels=True):\n",
        "  \"\"\"\n",
        "  Inputs:\n",
        "    x - torch.Tensor representing the image of shape [B, C, H, W]\n",
        "    patch_size - Number of pixels per dimension of the patches (integer)\n",
        "    flatten_channels - If True, the patches will be returned in a flattened format\n",
        "                        as a feature vector instead of a image grid.\n",
        "  \"\"\"\n",
        "  B, C, H, W = x.shape\n",
        "  x = x.reshape(B, C, H//patch_size, patch_size, W//patch_size, patch_size)\n",
        "  x = x.permute(0, 2, 4, 1, 3, 5) # [B, H', W', C, p_H, p_W]\n",
        "  x = x.flatten(1,2)              # [B, H'*W', C, p_H, p_W]\n",
        "  if flatten_channels:\n",
        "    x = x.flatten(2,4)          # [B, H'*W', C*p_H*p_W]\n",
        "  return x"
      ],
      "metadata": {
        "id": "ImvTeTDFsRc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualizing Patches of example_images\n",
        "#@markdown Change the patch size to view the resulting image patches.\n",
        "#@markdown Make sure the image can be fully split into the desired size of patches.\n",
        "\n",
        "# Define image dimensions and patch size\n",
        "image_dim = example_images.shape[2]\n",
        "patch_size = 4  #@param {type: \"number\"}\n",
        "\n",
        "# Convert example_images to patches\n",
        "img_patches = img_to_patch(example_images, patch_size=patch_size, flatten_channels=False)\n",
        "\n",
        "# Calculate the number of patches per row dynamically\n",
        "num_patches_per_row = image_dim // patch_size\n",
        "\n",
        "# Adjust the visualization dynamically\n",
        "fig, ax = plt.subplots(img_patches.shape[0], 1, figsize=(14, 3 * img_patches.shape[0]))\n",
        "fig.suptitle(\"Images as input sequences of patches\", fontsize=16)\n",
        "for i in range(img_patches.shape[0]):\n",
        "    # img_patches[i] has shape [H'*W', C, patch_size, patch_size]\n",
        "    img_grid = torchvision.utils.make_grid(\n",
        "        img_patches[i].reshape(-1, *img_patches.shape[2:]),  # Reshape to [num_patches, C, patch_size, patch_size]\n",
        "        nrow=num_patches_per_row,  # Calculate patches per row dynamically\n",
        "        normalize=True,\n",
        "        pad_value=0.9\n",
        "    )\n",
        "    img_grid = img_grid.permute(1, 2, 0)  # Convert to HWC format for visualization\n",
        "    ax[i].imshow(img_grid)\n",
        "    ax[i].axis(\"off\")\n",
        "plt.show()\n",
        "plt.close()\n"
      ],
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "-lTCqpHwmKML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualizing Patches of example_images in a sequence\n",
        "#@markdown Change the patch size to view the resulting image patches.\n",
        "#@markdown Make sure the image can be fully split into the desired size of patches.\n",
        "\n",
        "# Define image dimensions and patch size\n",
        "image_dim = example_images.shape[2]\n",
        "patch_size = 4  #@param {type: \"number\"}\n",
        "\n",
        "# Convert images to patches with the specified patch size\n",
        "img_patches = img_to_patch(example_images, patch_size=patch_size, flatten_channels=False)\n",
        "\n",
        "# Visualize the patches in a single row\n",
        "fig, ax = plt.subplots(example_images.shape[0], 1, figsize=(14, 3 * example_images.shape[0]))\n",
        "\n",
        "for i in range(example_images.shape[0]):\n",
        "    # Calculate the number of patches in the row for the current image\n",
        "    num_patches = img_patches[i].shape[0]\n",
        "\n",
        "    # Row visualization (all patches in a single row)\n",
        "    img_row = torchvision.utils.make_grid(\n",
        "        img_patches[i].reshape(-1, *img_patches.shape[2:]),  # Reshape to [num_patches, C, patch_size, patch_size]\n",
        "        nrow=num_patches,  # Show all patches in a single row\n",
        "        normalize=True,\n",
        "        pad_value=0.9\n",
        "    )\n",
        "\n",
        "    # Convert to HWC format for visualization\n",
        "    img_row = img_row.permute(1, 2, 0)\n",
        "\n",
        "    # Plot the image row\n",
        "    ax[i].imshow(img_row)\n",
        "    ax[i].axis(\"off\")\n",
        "    ax[i].set_title(f\"Image {i + 1}: Patches in Row\", fontsize=12)\n",
        "\n",
        "# Adjust layout and show the plot\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "plt.show()\n",
        "plt.close()\n"
      ],
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "UbjLMSL1nmts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 3: Vision Transformer\n"
      ],
      "metadata": {
        "id": "j08GzFc1oXmb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can start building the Transformer model. The model consists of (in this order):\n",
        "\n",
        "* A **linear projection** layer\n",
        "\n",
        "  that maps the input patches to a feature vector of larger size. It is implemented by a simple linear layer that takes each $ M\\times M $ patch independently as input.\n",
        "\n",
        "* A **classification token**\n",
        "\n",
        "  that is added to the input sequence. The CLS (classification) token is a special learnable embedding added at the start of the sequence of patches.\n",
        "  It acts as a summary representation for the entire image. After processing by the Transformer layers, the CLS token is expected to contain the most relevant information for the classification task.\n",
        "\n",
        "* Learnable **positional encodings**\n",
        "\n",
        "  that are added to the tokens before being processed by the Transformer. Those are needed to learn position-dependent information, and convert the set to a sequence. Since we usually work with a fixed resolution, we can learn the positional encodings instead of having the pattern of sine and cosine functions.\n",
        "\n",
        "* A **Transformer Encoder/Block**\n",
        "\n",
        "  discussed in detain in the next subsection. The block is repeated multiple times.\n",
        "\n",
        "* An **MLP head**\n",
        "\n",
        "  that takes the output feature vector of the CLS token, and maps it to a classification prediction. This is usually implemented by a small feed-forward network or even a single linear layer.\n",
        "\n",
        "\n",
        "The figure below contains all the explained pieces."
      ],
      "metadata": {
        "id": "P0oGC3XWC_SQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center width=\"100%\"><img src=\"https://d2l.ai/_images/vit.svg\" width=\"650px\"></center>"
      ],
      "metadata": {
        "id": "HiEAQWPw4w0V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1. The Attention Block"
      ],
      "metadata": {
        "id": "0otCWD3HIMtg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The attention block includes the following components :\n",
        "\n",
        "1. Layer Normalization: There are two layer normalizations\n",
        "    - The first one normalizes the input before the attention block.\n",
        "    - The second one normalizes the input before the feed-forward network.\n",
        "\n",
        "2. Multi-Head Attention: There is one multihead attention for self-attention, with the following parameters:\n",
        "    - `embed_dim`: Dimensionality of input vectors.\n",
        "    - `num_heads`: Number of attention heads.\n",
        "    - Apply dropout as part of the attention mechanism.\n",
        "\n",
        "3. Feed-Forward Network (FFN):Design a feed-forward network that includes (in this order):\n",
        "    - First linear layer projects from `embed_dim` to `hidden_dim`.\n",
        "    - GELU Activation function.\n",
        "    - Dropout.\n",
        "    - Second linear layer projects from `hidden_dim` back to `embed_dim`.\n",
        "    - Another dropout.\n",
        "\n",
        "4. Residual Connections: The model has two connections\n",
        "    - Combine the input with the output of the multihead attention.\n",
        "    - Combine the result of the previous combination with the output of the FFN.\n",
        "\n",
        "The inputs to the implemented model are:\n",
        "- `embed_dim`: Dimensionality of input and attention feature vectors.\n",
        "- `hidden_dim`: Dimensionality of the hidden layer in the feed-forward network (usually 2-4x larger than `embed_dim`).\n",
        "- `num_heads`: Number of heads for the Multi-Head Attention block.\n",
        "- `dropout`: Amount of dropout to apply in the feed-forward network. (both in dropout layers and attention)\n",
        "\n",
        "The Forward Pass:\n",
        "1. Normalize the input and pass it through the attention mechanism.\n",
        "2. Add a residual connection from the input to the attention output.\n",
        "3. Normalize the attention output and pass it through the feed-forward network.\n",
        "4. Add a residual connection from the attention output to the feed-forward output.\n",
        "\n",
        "Check out the ilustration below to further enhance your understanding of the attention block. Your task in this subsection is\n",
        "\n",
        "> Fill the `__init__` and `forward` methods in the `AttentionBlock` class."
      ],
      "metadata": {
        "id": "niAcZd35IZmp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://discuss.d2l.ai/uploads/default/optimized/2X/e/e635a8fb7898d1c260a5a0d5e1fde010801d6ee8_2_690x418.png\" width=\"450px\"></center>"
      ],
      "metadata": {
        "id": "JFPBygFSEsJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, embed_dim, hidden_dim, num_heads, dropout=0.0):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      embed_dim - Dimensionality of input and attention feature vectors\n",
        "      hidden_dim - Dimensionality of hidden layer in feed-forward network\n",
        "                    (usually 2-4x larger than embed_dim)\n",
        "      num_heads - Number of heads to use in the Multi-Head Attention block\n",
        "      dropout - Amount of dropout to apply in the feed-forward network\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    return"
      ],
      "metadata": {
        "id": "rJx9E-po46Cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2. Vision Transformer"
      ],
      "metadata": {
        "id": "Y26TiCf7MaR7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will implement the full Vision Transformer, using the AttentionBlock created above and adding the other pieces: A linear projection layer, a classification token, positional encodings and a MLP head.\n",
        "\n",
        "The class below is partially implemented to include all the pieces."
      ],
      "metadata": {
        "id": "SofVv_y5NIM4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the VisionTransformer class step-by-step by following these guidelines:\n",
        "\n",
        "1. **Initialization (`__init__`)**:\n",
        "  - **Patch Embedding**: A linear layer is used to map the flattened patches to the embedding dim.\n",
        "  - **Transformer Layers**: A stack of `AttentionBlock` layers, the number of layers is defined by `num_layers` parameter.\n",
        "  - **Classification Head**: A feed-forward network using `nn.LayerNorm` and `nn.Linear`. which maps the final `CLS` token to the output logits.\n",
        "  - **Positional Embedding**: Define a `nn.Parameter` for positional encoding.\n",
        "  - **CLS Token**: Define a `nn.Parameter` for the `CLS` token.\n",
        "  - **Dropout**: for regularization with the given `dropout` rate.\n",
        "\n",
        "2. **Forward Pass (`forward`)**:\n",
        "    - **Step 1**: Convert the input images into patches using the provided `img_to_patch` function.\n",
        "        - Ensure the resulting tensor is of shape `[B, T, embed_dim]` (batch, patches, embedding).\n",
        "    - **Step 2**: Apply the patch embedding layer.\n",
        "    - **Step 3**: Add the `CLS` token to the beginning of the sequence for each image in the batch.\n",
        "    - **Step 4**: Add positional encodings to the sequence.\n",
        "    - **Step 5**: Apply a dropout layer (missing in the viz above)\n",
        "    - **Step 6**: Pass the input through the Transformer layers (`AttentionBlock` stack).\n",
        "        - Note: Ensure the sequence matches the order the attention block needs.\n",
        "    - **Step 7**: Use the `CLS` token output to compute class logits via the classification head.\n",
        "\n",
        "#### Inputs:\n",
        "- `x`: Input tensor of shape `[B, C, H, W]` where:\n",
        "    - `B`: Batch size\n",
        "    - `C`: Number of input channels (e.g., 3 for RGB).\n",
        "    - `H` and `W`: Height and width of the images.\n",
        "\n",
        "#### Outputs:\n",
        "- `out`: Tensor of shape `[B, num_classes]`, representing the class logits for each image in the batch.\n",
        "\n",
        "Your task in this section is:\n",
        "> Fill the missing layer and parameter sizes in the `__init__`."
      ],
      "metadata": {
        "id": "WNRHbyw3U5PA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "\n",
        "  def __init__(self, embed_dim, hidden_dim, num_channels, num_heads, num_layers, num_classes, patch_size, num_patches, dropout=0.0):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      embed_dim - Dimensionality of the input feature vectors to the Transformer\n",
        "      hidden_dim - Dimensionality of the hidden layer in the feed-forward networks\n",
        "                    within the Transformer\n",
        "      num_channels - Number of channels of the input\n",
        "      num_heads - Number of heads to use in the Multi-Head Attention block\n",
        "      num_layers - Number of layers to use in the AttentionBlock\n",
        "      num_classes - Number of classes to predict\n",
        "      patch_size - Number of pixels that the patches have per dimension\n",
        "      num_patches - Maximum number of patches an image can have\n",
        "      dropout - Amount of dropout to apply in the feed-forward network and\n",
        "                on the input encoding\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.patch_size = patch_size\n",
        "\n",
        "    self.input_layer = nn.Linear( , )\n",
        "\n",
        "    self.cls_token = nn.Parameter(torch.randn(, , ))\n",
        "    self.pos_embedding = nn.Parameter(torch.randn(, , ))\n",
        "\n",
        "    self.transformer = nn.Sequential(*[AttentionBlock(, , , ) for _ in range(num_layers)])\n",
        "\n",
        "    self.mlp_head = nn.Sequential(\n",
        "      nn.LayerNorm(),\n",
        "      nn.Linear(, )\n",
        "    )\n",
        "    self.dropout = nn.Dropout()\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Preprocess input\n",
        "    x = img_to_patch(x, self.patch_size)\n",
        "    B, T, _ = x.shape\n",
        "    x = self.input_layer(x)\n",
        "\n",
        "    # Add CLS token and positional encoding\n",
        "    cls_token = self.cls_token.repeat(B, 1, 1)\n",
        "    x = torch.cat([cls_token, x], dim=1)\n",
        "    x = x + self.pos_embedding[:,:T+1]\n",
        "\n",
        "    # Apply Transformer\n",
        "    x = self.dropout(x)\n",
        "    x = x.transpose(0, 1)\n",
        "    x = self.transformer(x)\n",
        "\n",
        "    # Perform classification prediction\n",
        "    cls = x[0]\n",
        "    out = self.mlp_head(cls)\n",
        "    return out"
      ],
      "metadata": {
        "id": "8tqJX1ge5En-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 4: Train and Evaluate the ViT\n",
        "\n"
      ],
      "metadata": {
        "id": "uSr1iJ4ZXFv6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section includes the model training, and testing. A `train_model` function is partially implemented to train, evaluate, save best model and then test it on the test set. Familiaraize yourself with the method, your tasks for this section are:\n",
        "\n",
        "> 1. Fill the missing code lines in the train_model function (in the model training part).\n",
        ">\n",
        "> 2. Fill the missing model arguments, loss function, optimizer and number of epochs. Then train the model.\n",
        ">    \n",
        ">    2.1. Train the model with at least 3 different patch sizes.\n",
        ">\n",
        "> 3. Compare the results of the training with the different patch sizes, reflect on the chnages in preformance.\n",
        "\n",
        "*Note* try to set parameters that allow training quickly, a min or two per epoch. Consider testing a set of parameters on a tiny training session (5-7 epochs) before running the final training."
      ],
      "metadata": {
        "id": "lmMgcB44cqeq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Seed everything for reproducibility\n",
        "def seed_everything(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Training and evaluation loop\n",
        "def train_model(model, train_loader, val_loader, test_loader, num_epochs=180):\n",
        "    seed_everything(42)\n",
        "\n",
        "    # Check device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Train the model from scratch\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss, train_acc = 0.0, 0.0\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "            #TODO add missing training steps here.\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            train_acc += (preds.argmax(dim=-1) == labels).float().mean().item()\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "        train_acc /= len(train_loader)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss, val_acc = 0.0, 0.0\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in val_loader:\n",
        "                imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "                preds = model(imgs)\n",
        "                loss = criterion(preds, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                val_acc += (preds.argmax(dim=-1) == labels).float().mean().item()\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_acc /= len(val_loader)\n",
        "\n",
        "        # Save the best model\n",
        "        best_model_path = \"best_model.pth\"\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "        print(f\"  Current LR: {current_lr:.6f}\")\n",
        "\n",
        "    # Test the model\n",
        "    print(\"Testing the best model...\")\n",
        "    model.load_state_dict(torch.load(best_model_path))\n",
        "    model.eval()\n",
        "    test_acc = 0.0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in test_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            preds = model(imgs)\n",
        "            test_acc += (preds.argmax(dim=-1) == labels).float().mean().item()\n",
        "    test_acc /= len(test_loader)\n",
        "\n",
        "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "    return model, {\"test\": test_acc, \"val\": best_val_acc}"
      ],
      "metadata": {
        "id": "fg14O68W5Tq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO fill the model arguments, and setup the optimizer and loss function.\n",
        "\n",
        "# Define your model parameters\n",
        "model_kwargs = {\n",
        "    'embed_dim': ,\n",
        "    'hidden_dim': ,\n",
        "    'num_heads': ,\n",
        "    'num_layers': ,\n",
        "    'patch_size': ,\n",
        "    'num_channels': ,\n",
        "    'num_patches': ,\n",
        "    'num_classes': ,\n",
        "    'dropout':\n",
        "}\n",
        "\n",
        "# Instantiate the model\n",
        "model = VisionTransformer(**model_kwargs).to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer =\n",
        "\n",
        "# Loss function\n",
        "criterion ="
      ],
      "metadata": {
        "id": "woZroXzc5V8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: set number of epochs and train the model.\n",
        "\n",
        "num_epochs =\n",
        "\n",
        "# Train the model\n",
        "model, results = train_model(model, train_loader, val_loader, test_loader, num_epochs=num_epochs)\n",
        "print(f\"Validation Accuracy: {results['val']:.4f}\")\n",
        "print(f\"Test Accuracy: {results['test']:.4f}\")"
      ],
      "metadata": {
        "id": "evlXnQ7q5X2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 5: CNN\n"
      ],
      "metadata": {
        "id": "iu3CO7kfXQGm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section you will suggest a CNN architecture to solve the same problem above, classify the cats and dogs breed in the given images. Detailed tasks:\n",
        "\n",
        "\n",
        "1.   Suggest a CNN model to solve the same problem the transformer above was trained on. Explain and implement your choice.\n",
        "2.   Do you think a different data preperation is needed for the different architecture? (the transformations does in Section 1). Explain you choice.\n",
        "3.   Train and evaluate the model - you can reuse the `train_model` method defined above. Explain your choice of loss function and optimizer."
      ],
      "metadata": {
        "id": "8IGkBpnOXSVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: CNN"
      ],
      "metadata": {
        "id": "2LF0GqFZYdgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 6: Compare & Discuss\n"
      ],
      "metadata": {
        "id": "HdpZyrYPYgbt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Compare the two architectures based on the following aspects:\n",
        "\n",
        "  *  Performance: Which model performs better on the test set? Are there significant differences in accuracy or other metrics?\n",
        "  *  Training Dynamics: Compare the training and validation curves. Which model converged faster? Did either model overfit?\n",
        "  *  Computational Efficiency: Compare the training and inference time of the models. Which model was more computationally demanding?\n",
        "\n",
        "2. Discussion\n",
        "\n",
        "  Based on your observations, discuss the trade-offs between Vision Transformers and CNNs:\n",
        "\n",
        "  *  Suitability for vision tasks: Which architecture seems better suited for the dataset and why?\n",
        "  *  Scalability: How might these results change with a larger dataset or higher-resolution images?"
      ],
      "metadata": {
        "id": "McbgcnLkYkKY"
      }
    }
  ]
}